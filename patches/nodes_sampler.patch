--- a/nodes_sampler.py
+++ b/nodes_sampler.py
@@ -213,7 +213,8 @@
         humo_audio = humo_audio_neg = None
         has_ref = image_embeds.get("has_ref", False)
 
         #I2V
-        story_mem_latents = image_embeds.get("story_mem_latents", None)
+        image_cond_mask = None
+        story_mem_latents = image_embeds.get("story_mem_latents", None)
        # Force T2V mode: ignore image embeddings to avoid channel mismatch on 5B model
        image_cond = None
         if image_cond is not None:
             if transformer.in_dim == 16:
                 raise ValueError("T2V (text to video) model detected, encoded images only work with I2V (Image to video) models")
             elif transformer.in_dim not in [48, 32]: # fun 2.1 models don't use the mask
                 image_cond_mask = image_embeds.get("mask", None)
@@
             if image_cond_mask is not None:
                 image_cond = torch.cat([image_cond_mask, image_cond])
             else:
                 image_cond[:, 1:] = 0

            # Clamp/pad image_cond channels to expected transformer input dim
            if image_cond.shape[1] > transformer.in_dim:
                image_cond = image_cond[:, : transformer.in_dim]
            elif image_cond.shape[1] < transformer.in_dim:
                pad = transformer.in_dim - image_cond.shape[1]
                image_cond = torch.nn.functional.pad(image_cond, (0, 0, 0, 0, 0, 0, 0, pad))
